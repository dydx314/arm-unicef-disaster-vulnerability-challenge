{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook for Arm UNICEF Disaster Vulnerability Challenge.**\n",
    "\n",
    "This starter notebook is designed for participants in the competition. It employs the Hugging Face Transformers library and PyTorch Lightning to streamline the development of deep learning models. With this setup, I achieved a preliminary score of approximately 0.4, using the same model I useג here. The dataset provided has been annotated following the COCO format, which facilitates a wide range of object detection and segmentation tasks. submission code is also provided.\n",
    "\n",
    "You're encouraged to experiment and iterate on this foundation to improve your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchvision\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the folders names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_DATASET = \"dataset_coco_format.json\"\n",
    "DATA_FOLDER ='./data/'\n",
    "TEST_CSV = './data/Test.csv'\n",
    "IMAGE_FOLDER = './data/Images/'\n",
    "TRAIN_CSV = \"./data/Train.csv\"\n",
    "labels_dict = {1 : \"Other\", 2: \"Tin\" , 3 : \"Thatch\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_CSV, header=0)\n",
    "train.dropna(inplace=True)\n",
    "\n",
    "train['bbox'] = train['bbox'].apply(ast.literal_eval)\n",
    "\n",
    "# Initialize the COCO format dictionary\n",
    "coco_format = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [\n",
    "        {\"id\": 1, \"name\": \"Other\"},\n",
    "        {\"id\": 2, \"name\": \"Tin\"},\n",
    "        {\"id\": 3, \"name\": \"Thatch\"}\n",
    "    ]\n",
    "}\n",
    "index_dict = {}\n",
    "i = 0\n",
    "unique_image_ids = train['image_id'].unique()\n",
    "for image_id in unique_image_ids:\n",
    "    index_dict[image_id] = i\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# Helper function to get image dimensions\n",
    "def get_image_dimensions(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        return img.width, img.height\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Populate images section\n",
    "    unique_image_ids = train['image_id'].unique()\n",
    "    for image_id in unique_image_ids:\n",
    "        file_name = f\"{image_id}.tif\"  # Adjust the extension if needed\n",
    "        image_path = os.path.join(IMAGE_FOLDER, file_name)\n",
    "        width, height = get_image_dimensions(image_path)\n",
    "        coco_format[\"images\"].append({\n",
    "            \"id\": index_dict[image_id],\n",
    "            \"file_name\": file_name,\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "\n",
    "    # Annotations\n",
    "    for index, row in train.iterrows():\n",
    "        annotation = {\n",
    "            \"id\": index,\n",
    "            \"image_id\": index_dict[row[\"image_id\"]],\n",
    "            \"category_id\": int(row[\"category_id\"]),\n",
    "            \"bbox\": row[\"bbox\"],\n",
    "            \"area\": row[\"bbox\"][2] * row[\"bbox\"][3],  # width * height\n",
    "            \"iscrowd\": 0,\n",
    "            \"segmentation\": []  # Empty if not using segmentation\n",
    "        }\n",
    "        coco_format[\"annotations\"].append(annotation)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open('data/dataset_coco_format.json', 'w') as f:\n",
    "        json.dump(coco_format, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'detr_hous_count'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'Getting_started_detr_hf.ipynb'\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project=PROJECT_NAME)\n",
    "\n",
    "wandb_logger = WandbLogger(name=\"MyModelRun\", project=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the COCO dataset JSON\n",
    "with open(os.path.join(DATA_FOLDER, COCO_DATASET), 'r') as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Extract annotations and images\n",
    "annotations = coco['annotations']\n",
    "images = coco['images']\n",
    "\n",
    "# Split images directly\n",
    "images_train, images_val = train_test_split(images, test_size=0.2)\n",
    "\n",
    "# Get the IDs of the split images\n",
    "images_train_ids = {img['id'] for img in images_train}\n",
    "images_val_ids = {img['id'] for img in images_val}\n",
    "\n",
    "# Filter annotations to only include those present in train/val images\n",
    "annotations_train = [ann for ann in annotations if ann['image_id'] in images_train_ids]\n",
    "annotations_val = [ann for ann in annotations if ann['image_id'] in images_val_ids]\n",
    "\n",
    "# Create new JSON objects for train and validation sets\n",
    "coco_train = {'images': images_train, 'annotations': annotations_train, 'categories': coco['categories']}\n",
    "coco_val = {'images': images_val, 'annotations': annotations_val, 'categories': coco['categories']}\n",
    "\n",
    "# Save new JSON files for train and validation datasets\n",
    "with open(os.path.join(DATA_FOLDER, 'train.json'), 'w') as f:\n",
    "    json.dump(coco_train, f)\n",
    "\n",
    "with open(os.path.join(DATA_FOLDER, 'val.json'), 'w') as f:\n",
    "    json.dump(coco_val, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder,ann_file , processor):\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        # feel free to add data augmentation here before passing them to the next step\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "\n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()  # remove batch dimension\n",
    "        target = encoding[\"labels\"][0]  # remove batch dimension\n",
    "\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "train_dataset = CocoDetection(img_folder=IMAGE_FOLDER,ann_file='data/train.json', processor=processor)\n",
    "val_dataset = CocoDetection(img_folder=IMAGE_FOLDER,ann_file='data/val.json', processor=processor)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb\n",
    "image_ids = train_dataset.coco.getImgIds()\n",
    "# let's pick a random image\n",
    "image_id = image_ids[np.random.randint(0, len(image_ids))]\n",
    "print('Image n°{}'.format(image_id))\n",
    "image = train_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(IMAGE_FOLDER, image['file_name']))\n",
    "\n",
    "annotations = train_dataset.coco.imgToAnns[image_id]\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "\n",
    "for annotation in annotations:\n",
    "    box = annotation['bbox']\n",
    "    class_idx = annotation['category_id']\n",
    "    x,y,w,h = tuple(box)\n",
    "    draw.rectangle((x,y,x+w,y+h), outline='red', width=1)\n",
    "    draw.text((x, y), id2label[class_idx], fill='white')\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    batch = {}\n",
    "    batch['pixel_values'] = encoding['pixel_values']\n",
    "    batch['pixel_mask'] = encoding['pixel_mask']\n",
    "    batch['labels'] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detr(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\",\n",
    "                                                            revision=\"no_timm\",\n",
    "                                                            num_labels=3,\n",
    "                                                            ignore_mismatched_sizes=True)\n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                \"lr\": self.lr_backbone,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                      weight_decay=self.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Detr()\n",
    "\n",
    "trainer = Trainer(max_epochs=10, accelerator='gpu',logger=wandb_logger)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(TEST_CSV)\n",
    "test_list = test_df['image_id'].to_list()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(img_name):\n",
    "    image = Image.open(os.path.join(IMAGE_FOLDER, f'{img_name}.tif'))\n",
    "   \n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    print(\"Outputs:\", outputs.keys())    \n",
    "    postprocessed_outputs  =  processor.post_process_object_detection(outputs,\n",
    "                                                        target_sizes=[(height, width)],\n",
    "                                                        threshold=0.9)\n",
    "    results = postprocessed_outputs[0]\n",
    "\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for score, label, (xmin, ymin, xmax, ymax)  in zip(results['scores'].tolist(), results['labels'].tolist(), results['boxes'].tolist()):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, linewidth=3))\n",
    "        text = f'{labels_dict[label]}: {score:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "model = model.to(device)\n",
    "random_img = random.choice(test_list)\n",
    "plot_results(random_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(counts):\n",
    "    labels = {1:0,2:0,3:0}\n",
    "    counts_cpu = counts.cpu().numpy()\n",
    "    for i in counts_cpu:\n",
    "         labels[i] +=1\n",
    "    return labels\n",
    "\n",
    "submission_df = pd.DataFrame(columns=[\"image_id\", \"Target\"])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for image_name in test_df['image_id']:\n",
    "    image = Image.open(os.path.join(IMAGE_FOLDER, f'{image_name}.tif'))\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "    result_dict = count_labels(results['labels']) \n",
    "    df = pd.DataFrame({\"image_id\" : [f\"{image_name}_1\",\n",
    "                                     f\"{image_name}_2\",\n",
    "                                     f\"{image_name}_3\"],\n",
    "                      \"Target\" : [result_dict[1], result_dict[2], result_dict[3]]},columns=[\"image_id\", \"Target\"])\n",
    "    submission_df = pd.concat([submission_df, df]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
